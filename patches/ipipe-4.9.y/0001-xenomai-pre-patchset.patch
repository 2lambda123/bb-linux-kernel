From 63e04f312c140f726e5b11a5aedc268e3912a8d8 Mon Sep 17 00:00:00 2001
From: Robert Nelson <robertcnelson@gmail.com>
Date: Wed, 7 Mar 2018 14:59:14 -0600
Subject: [PATCH 1/2] xenomai pre-patchset

---
 arch/arm/mach-omap2/timer.c        | 121 +---------------
 arch/powerpc/kernel/align.c        | 119 ++++++---------
 arch/x86/entry/common.c            |   9 +-
 arch/x86/entry/entry_32.S          |  22 +--
 arch/x86/entry/entry_64.S          | 290 +++++++++++++++++--------------------
 arch/x86/include/asm/thread_info.h |  14 +-
 drivers/gpio/gpio-davinci.c        | 218 +++++++++++++---------------
 7 files changed, 297 insertions(+), 496 deletions(-)

diff --git a/arch/arm/mach-omap2/timer.c b/arch/arm/mach-omap2/timer.c
index 7aed750a341a..b2f2448bfa6d 100644
--- a/arch/arm/mach-omap2/timer.c
+++ b/arch/arm/mach-omap2/timer.c
@@ -33,7 +33,6 @@
 #include <linux/clk.h>
 #include <linux/delay.h>
 #include <linux/irq.h>
-#include <linux/irqchip/arm-gic.h>
 #include <linux/clocksource.h>
 #include <linux/clockchips.h>
 #include <linux/slab.h>
@@ -64,20 +63,11 @@
 #define INCREMENTER_DENUMERATOR_RELOAD_OFFSET		0x14
 #define NUMERATOR_DENUMERATOR_MASK			0xfffff000
 
-#define AM43XX_GIC_CPU_BASE				0x48240100
-
-static void __iomem *gic_cpu_base;
-
 /* Clockevent code */
 
 static struct omap_dm_timer clkev;
 static struct clock_event_device clockevent_gpt;
 
-/* Clockevent hwmod for am335x and am437x suspend */
-struct omap_hwmod *clockevent_gpt_hwmod;
-static struct irq_chip *clkev_irq_chip;
-static struct irq_desc *clkev_irq_desc;
-
 #ifdef CONFIG_SOC_HAS_REALTIME_COUNTER
 static unsigned long arch_timer_freq;
 
@@ -135,62 +125,6 @@ static int omap2_gp_timer_set_periodic(struct clock_event_device *evt)
 	return 0;
 }
 
-static int omap_clkevt_late_ack_init(void)
-{
-	gic_cpu_base = ioremap(AM43XX_GIC_CPU_BASE, SZ_4K);
-
-	if (!gic_cpu_base)
-		return -ENOMEM;
-
-	return 0;
-}
-
-static void omap_clkevt_late_ack(void)
-{
-	u32 val;
-
-	if (!clkev_irq_chip)
-		return;
-
-	/*
-	 * For the gic to properly clear an interrupt it must be read
-	 * from INTACK register
-	 */
-	if (gic_cpu_base)
-		val = readl_relaxed(gic_cpu_base + GIC_CPU_INTACK);
-	if (clkev_irq_chip->irq_ack)
-		clkev_irq_chip->irq_ack(&clkev_irq_desc->irq_data);
-	if (clkev_irq_chip->irq_eoi)
-		clkev_irq_chip->irq_eoi(&clkev_irq_desc->irq_data);
-
-	clkev_irq_chip->irq_unmask(&clkev_irq_desc->irq_data);
-}
-
-static void omap_clkevt_idle(struct clock_event_device *unused)
-{
-	if (!clockevent_gpt_hwmod)
-		return;
-
-	/*
-	 * It is possible for a late interrupt to be generated which will
-	 * cause a suspend failure. Let's ack it here both in the timer
-	 * and the interrupt controller to avoid this.
-	 */
-	__omap_dm_timer_write_status(&clkev, OMAP_TIMER_INT_OVERFLOW);
-	omap_clkevt_late_ack();
-
-	omap_hwmod_idle(clockevent_gpt_hwmod);
-}
-
-static void omap_clkevt_unidle(struct clock_event_device *unused)
-{
-	if (!clockevent_gpt_hwmod)
-		return;
-
-	omap_hwmod_enable(clockevent_gpt_hwmod);
-	__omap_dm_timer_int_enable(&clkev, OMAP_TIMER_INT_OVERFLOW);
-}
-
 static struct clock_event_device clockevent_gpt = {
 	.features		= CLOCK_EVT_FEAT_PERIODIC |
 				  CLOCK_EVT_FEAT_ONESHOT,
@@ -424,22 +358,6 @@ static void __init omap2_gp_clockevent_init(int gptimer_id,
 					3, /* Timer internal resynch latency */
 					0xffffffff);
 
-	if (soc_is_am33xx() || soc_is_am43xx()) {
-		clockevent_gpt.suspend = omap_clkevt_idle;
-		clockevent_gpt.resume = omap_clkevt_unidle;
-
-		clockevent_gpt_hwmod =
-			omap_hwmod_lookup(clockevent_gpt.name);
-
-		clkev_irq_desc = irq_to_desc(clkev.irq);
-		if (clkev_irq_desc)
-			clkev_irq_chip = irq_desc_get_chip(clkev_irq_desc);
-
-	}
-
-	if (soc_is_am437x())
-		omap_clkevt_late_ack_init();
-
 	pr_info("OMAP clockevent source: %s at %lu Hz\n", clockevent_gpt.name,
 		clkev.rate);
 }
@@ -458,7 +376,7 @@ static cycle_t clocksource_read_cycles(struct clocksource *cs)
 }
 
 static struct clocksource clocksource_gpt = {
-	.rating		= 290,
+	.rating		= 300,
 	.read		= clocksource_read_cycles,
 	.mask		= CLOCKSOURCE_MASK(32),
 	.flags		= CLOCK_SOURCE_IS_CONTINUOUS,
@@ -531,38 +449,6 @@ static int __init __maybe_unused omap2_sync32k_clocksource_init(void)
 	return ret;
 }
 
-static unsigned int omap2_gptimer_clksrc_load;
-
-static void omap2_gptimer_clksrc_suspend(struct clocksource *unused)
-{
-	struct omap_hwmod *oh;
-
-	omap2_gptimer_clksrc_load =
-		__omap_dm_timer_read_counter(&clksrc, OMAP_TIMER_NONPOSTED);
-
-	oh = omap_hwmod_lookup(clocksource_gpt.name);
-	if (!oh)
-		return;
-
-	omap_hwmod_idle(oh);
-}
-
-static void omap2_gptimer_clksrc_resume(struct clocksource *unused)
-{
-	struct omap_hwmod *oh;
-
-	oh = omap_hwmod_lookup(clocksource_gpt.name);
-	if (!oh)
-		return;
-
-	omap_hwmod_enable(oh);
-
-	__omap_dm_timer_load_start(&clksrc,
-				   OMAP_TIMER_CTRL_ST | OMAP_TIMER_CTRL_AR,
-				   omap2_gptimer_clksrc_load,
-				   OMAP_TIMER_NONPOSTED);
-}
-
 static void __init omap2_gptimer_clocksource_init(int gptimer_id,
 						  const char *fck_source,
 						  const char *property)
@@ -572,11 +458,6 @@ static void __init omap2_gptimer_clocksource_init(int gptimer_id,
 	clksrc.id = gptimer_id;
 	clksrc.errata = omap_dm_timer_get_errata();
 
-	if (soc_is_am43xx()) {
-		clocksource_gpt.suspend = omap2_gptimer_clksrc_suspend;
-		clocksource_gpt.resume = omap2_gptimer_clksrc_resume;
-	}
-
 	res = omap_dm_timer_init_one(&clksrc, fck_source, property,
 				     &clocksource_gpt.name,
 				     OMAP_TIMER_NONPOSTED);
diff --git a/arch/powerpc/kernel/align.c b/arch/powerpc/kernel/align.c
index 292458b694fb..b2da7c8baed7 100644
--- a/arch/powerpc/kernel/align.c
+++ b/arch/powerpc/kernel/align.c
@@ -235,28 +235,6 @@ static int emulate_dcbz(struct pt_regs *regs, unsigned char __user *addr)
 
 #define SWIZ_PTR(p)		((unsigned char __user *)((p) ^ swiz))
 
-#define __get_user_or_set_dar(_regs, _dest, _addr)		\
-	({							\
-		int rc = 0;					\
-		typeof(_addr) __addr = (_addr);			\
-		if (__get_user_inatomic(_dest, __addr)) {	\
-			_regs->dar = (unsigned long)__addr;	\
-			rc = -EFAULT;				\
-		}						\
-		rc;						\
-	})
-
-#define __put_user_or_set_dar(_regs, _src, _addr)		\
-	({							\
-		int rc = 0;					\
-		typeof(_addr) __addr = (_addr);			\
-		if (__put_user_inatomic(_src, __addr)) {	\
-			_regs->dar = (unsigned long)__addr;	\
-			rc = -EFAULT;				\
-		}						\
-		rc;						\
-	})
-
 static int emulate_multiple(struct pt_regs *regs, unsigned char __user *addr,
 			    unsigned int reg, unsigned int nb,
 			    unsigned int flags, unsigned int instr,
@@ -285,10 +263,9 @@ static int emulate_multiple(struct pt_regs *regs, unsigned char __user *addr,
 		} else {
 			unsigned long pc = regs->nip ^ (swiz & 4);
 
-			if (__get_user_or_set_dar(regs, instr,
-						  (unsigned int __user *)pc))
+			if (__get_user_inatomic(instr,
+						(unsigned int __user *)pc))
 				return -EFAULT;
-
 			if (swiz == 0 && (flags & SW))
 				instr = cpu_to_le32(instr);
 			nb = (instr >> 11) & 0x1f;
@@ -332,31 +309,31 @@ static int emulate_multiple(struct pt_regs *regs, unsigned char __user *addr,
 			       ((nb0 + 3) / 4) * sizeof(unsigned long));
 
 		for (i = 0; i < nb; ++i, ++p)
-			if (__get_user_or_set_dar(regs, REG_BYTE(rptr, i ^ bswiz),
-						  SWIZ_PTR(p)))
+			if (__get_user_inatomic(REG_BYTE(rptr, i ^ bswiz),
+						SWIZ_PTR(p)))
 				return -EFAULT;
 		if (nb0 > 0) {
 			rptr = &regs->gpr[0];
 			addr += nb;
 			for (i = 0; i < nb0; ++i, ++p)
-				if (__get_user_or_set_dar(regs,
-							  REG_BYTE(rptr, i ^ bswiz),
-							  SWIZ_PTR(p)))
+				if (__get_user_inatomic(REG_BYTE(rptr,
+								 i ^ bswiz),
+							SWIZ_PTR(p)))
 					return -EFAULT;
 		}
 
 	} else {
 		for (i = 0; i < nb; ++i, ++p)
-			if (__put_user_or_set_dar(regs, REG_BYTE(rptr, i ^ bswiz),
-						  SWIZ_PTR(p)))
+			if (__put_user_inatomic(REG_BYTE(rptr, i ^ bswiz),
+						SWIZ_PTR(p)))
 				return -EFAULT;
 		if (nb0 > 0) {
 			rptr = &regs->gpr[0];
 			addr += nb;
 			for (i = 0; i < nb0; ++i, ++p)
-				if (__put_user_or_set_dar(regs,
-							  REG_BYTE(rptr, i ^ bswiz),
-							  SWIZ_PTR(p)))
+				if (__put_user_inatomic(REG_BYTE(rptr,
+								 i ^ bswiz),
+							SWIZ_PTR(p)))
 					return -EFAULT;
 		}
 	}
@@ -368,32 +345,29 @@ static int emulate_multiple(struct pt_regs *regs, unsigned char __user *addr,
  * Only POWER6 has these instructions, and it does true little-endian,
  * so we don't need the address swizzling.
  */
-static int emulate_fp_pair(struct pt_regs *regs, unsigned char __user *addr,
-			   unsigned int reg, unsigned int flags)
+static int emulate_fp_pair(unsigned char __user *addr, unsigned int reg,
+			   unsigned int flags)
 {
 	char *ptr0 = (char *) &current->thread.TS_FPR(reg);
 	char *ptr1 = (char *) &current->thread.TS_FPR(reg+1);
-	int i, sw = 0;
+	int i, ret, sw = 0;
 
 	if (reg & 1)
 		return 0;	/* invalid form: FRS/FRT must be even */
 	if (flags & SW)
 		sw = 7;
-
+	ret = 0;
 	for (i = 0; i < 8; ++i) {
 		if (!(flags & ST)) {
-			if (__get_user_or_set_dar(regs, ptr0[i^sw], addr + i))
-				return -EFAULT;
-			if (__get_user_or_set_dar(regs, ptr1[i^sw], addr + i + 8))
-				return -EFAULT;
+			ret |= __get_user(ptr0[i^sw], addr + i);
+			ret |= __get_user(ptr1[i^sw], addr + i + 8);
 		} else {
-			if (__put_user_or_set_dar(regs, ptr0[i^sw], addr + i))
-				return -EFAULT;
-			if (__put_user_or_set_dar(regs, ptr1[i^sw], addr + i + 8))
-				return -EFAULT;
+			ret |= __put_user(ptr0[i^sw], addr + i);
+			ret |= __put_user(ptr1[i^sw], addr + i + 8);
 		}
 	}
-
+	if (ret)
+		return -EFAULT;
 	return 1;	/* exception handled and fixed up */
 }
 
@@ -403,27 +377,24 @@ static int emulate_lq_stq(struct pt_regs *regs, unsigned char __user *addr,
 {
 	char *ptr0 = (char *)&regs->gpr[reg];
 	char *ptr1 = (char *)&regs->gpr[reg+1];
-	int i, sw = 0;
+	int i, ret, sw = 0;
 
 	if (reg & 1)
 		return 0;	/* invalid form: GPR must be even */
 	if (flags & SW)
 		sw = 7;
-
+	ret = 0;
 	for (i = 0; i < 8; ++i) {
 		if (!(flags & ST)) {
-			if (__get_user_or_set_dar(regs, ptr0[i^sw], addr + i))
-				return -EFAULT;
-			if (__get_user_or_set_dar(regs, ptr1[i^sw], addr + i + 8))
-				return -EFAULT;
+			ret |= __get_user(ptr0[i^sw], addr + i);
+			ret |= __get_user(ptr1[i^sw], addr + i + 8);
 		} else {
-			if (__put_user_or_set_dar(regs, ptr0[i^sw], addr + i))
-				return -EFAULT;
-			if (__put_user_or_set_dar(regs, ptr1[i^sw], addr + i + 8))
-				return -EFAULT;
+			ret |= __put_user(ptr0[i^sw], addr + i);
+			ret |= __put_user(ptr1[i^sw], addr + i + 8);
 		}
 	}
-
+	if (ret)
+		return -EFAULT;
 	return 1;	/* exception handled and fixed up */
 }
 #endif /* CONFIG_PPC64 */
@@ -716,14 +687,9 @@ static int emulate_vsx(unsigned char __user *addr, unsigned int reg,
 	for (j = 0; j < length; j += elsize) {
 		for (i = 0; i < elsize; ++i) {
 			if (flags & ST)
-				ret = __put_user_or_set_dar(regs, ptr[i^sw],
-							    addr + i);
+				ret |= __put_user(ptr[i^sw], addr + i);
 			else
-				ret = __get_user_or_set_dar(regs, ptr[i^sw],
-							    addr + i);
-
-			if (ret)
-				return ret;
+				ret |= __get_user(ptr[i^sw], addr + i);
 		}
 		ptr  += elsize;
 #ifdef __LITTLE_ENDIAN__
@@ -773,7 +739,7 @@ int fix_alignment(struct pt_regs *regs)
 	unsigned int dsisr;
 	unsigned char __user *addr;
 	unsigned long p, swiz;
-	int i;
+	int ret, i;
 	union data {
 		u64 ll;
 		double dd;
@@ -970,7 +936,7 @@ int fix_alignment(struct pt_regs *regs)
 		if (flags & F) {
 			/* Special case for 16-byte FP loads and stores */
 			PPC_WARN_ALIGNMENT(fp_pair, regs);
-			return emulate_fp_pair(regs, addr, reg, flags);
+			return emulate_fp_pair(addr, reg, flags);
 		} else {
 #ifdef CONFIG_PPC64
 			/* Special case for 16-byte loads and stores */
@@ -1000,12 +966,15 @@ int fix_alignment(struct pt_regs *regs)
 		}
 
 		data.ll = 0;
+		ret = 0;
 		p = (unsigned long)addr;
 
 		for (i = 0; i < nb; i++)
-			if (__get_user_or_set_dar(regs, data.v[start + i],
-						  SWIZ_PTR(p++)))
-				return -EFAULT;
+			ret |= __get_user_inatomic(data.v[start + i],
+						   SWIZ_PTR(p++));
+
+		if (unlikely(ret))
+			return -EFAULT;
 
 	} else if (flags & F) {
 		data.ll = current->thread.TS_FPR(reg);
@@ -1077,13 +1046,15 @@ int fix_alignment(struct pt_regs *regs)
 			break;
 		}
 
+		ret = 0;
 		p = (unsigned long)addr;
 
 		for (i = 0; i < nb; i++)
-			if (__put_user_or_set_dar(regs, data.v[start + i],
-						  SWIZ_PTR(p++)))
-				return -EFAULT;
+			ret |= __put_user_inatomic(data.v[start + i],
+						   SWIZ_PTR(p++));
 
+		if (unlikely(ret))
+			return -EFAULT;
 	} else if (flags & F)
 		current->thread.TS_FPR(reg) = data.ll;
 	else
diff --git a/arch/x86/entry/common.c b/arch/x86/entry/common.c
index b0cd306dc527..bdd9cc59d20f 100644
--- a/arch/x86/entry/common.c
+++ b/arch/x86/entry/common.c
@@ -20,7 +20,6 @@
 #include <linux/export.h>
 #include <linux/context_tracking.h>
 #include <linux/user-return-notifier.h>
-#include <linux/nospec.h>
 #include <linux/uprobes.h>
 
 #include <asm/desc.h>
@@ -202,7 +201,7 @@ __visible inline void prepare_exit_to_usermode(struct pt_regs *regs)
 	 * special case only applies after poking regs and before the
 	 * very next return to user mode.
 	 */
-	ti->status &= ~(TS_COMPAT|TS_I386_REGS_POKED);
+	current->thread.status &= ~(TS_COMPAT|TS_I386_REGS_POKED);
 #endif
 
 	user_enter_irqoff();
@@ -278,8 +277,7 @@ __visible void do_syscall_64(struct pt_regs *regs)
 	 * regs->orig_ax, which changes the behavior of some syscalls.
 	 */
 	if (likely((nr & __SYSCALL_MASK) < NR_syscalls)) {
-		nr = array_index_nospec(nr & __SYSCALL_MASK, NR_syscalls);
-		regs->ax = sys_call_table[nr](
+		regs->ax = sys_call_table[nr & __SYSCALL_MASK](
 			regs->di, regs->si, regs->dx,
 			regs->r10, regs->r8, regs->r9);
 	}
@@ -301,7 +299,7 @@ static __always_inline void do_syscall_32_irqs_on(struct pt_regs *regs)
 	unsigned int nr = (unsigned int)regs->orig_ax;
 
 #ifdef CONFIG_IA32_EMULATION
-	ti->status |= TS_COMPAT;
+	current->thread.status |= TS_COMPAT;
 #endif
 
 	if (READ_ONCE(ti->flags) & _TIF_WORK_SYSCALL_ENTRY) {
@@ -315,7 +313,6 @@ static __always_inline void do_syscall_32_irqs_on(struct pt_regs *regs)
 	}
 
 	if (likely(nr < IA32_NR_syscalls)) {
-		nr = array_index_nospec(nr, IA32_NR_syscalls);
 		/*
 		 * It's possible that a 32-bit syscall implementation
 		 * takes a 64-bit parameter but nonetheless assumes that
diff --git a/arch/x86/entry/entry_32.S b/arch/x86/entry/entry_32.S
index f5434b4670c1..edba8606b99a 100644
--- a/arch/x86/entry/entry_32.S
+++ b/arch/x86/entry/entry_32.S
@@ -45,7 +45,6 @@
 #include <asm/asm.h>
 #include <asm/smap.h>
 #include <asm/export.h>
-#include <asm/nospec-branch.h>
 
 	.section .entry.text, "ax"
 
@@ -229,18 +228,6 @@ ENTRY(__switch_to_asm)
 	movl	%ebx, PER_CPU_VAR(stack_canary)+stack_canary_offset
 #endif
 
-#ifdef CONFIG_RETPOLINE
-	/*
-	 * When switching from a shallower to a deeper call stack
-	 * the RSB may either underflow or use entries populated
-	 * with userspace addresses. On CPUs where those concerns
-	 * exist, overwrite the RSB with entries which capture
-	 * speculative execution to prevent attack.
-	 */
-	/* Clobbers %ebx */
-	FILL_RETURN_BUFFER RSB_CLEAR_LOOPS, X86_FEATURE_RSB_CTXSW
-#endif
-
 	/* restore callee-saved registers */
 	popl	%esi
 	popl	%edi
@@ -273,7 +260,7 @@ ENTRY(ret_from_fork)
 
 	/* kernel thread */
 1:	movl	%edi, %eax
-	CALL_NOSPEC %ebx
+	call	*%ebx
 	/*
 	 * A kernel thread is allowed to return here after successfully
 	 * calling do_execve().  Exit to userspace to complete the execve()
@@ -997,8 +984,7 @@ trace:
 	movl	0x4(%ebp), %edx
 	subl	$MCOUNT_INSN_SIZE, %eax
 
-	movl    ftrace_trace_function, %ecx
-	CALL_NOSPEC %ecx
+	call	*ftrace_trace_function
 
 	popl	%edx
 	popl	%ecx
@@ -1034,7 +1020,7 @@ return_to_handler:
 	movl	%eax, %ecx
 	popl	%edx
 	popl	%eax
-	JMP_NOSPEC %ecx
+	jmp	*%ecx
 #endif
 
 #ifdef CONFIG_TRACING
@@ -1076,7 +1062,7 @@ error_code:
 	movl	%ecx, %es
 	TRACE_IRQS_OFF
 	movl	%esp, %eax			# pt_regs pointer
-	CALL_NOSPEC %edi
+	call	*%edi
 	jmp	ret_from_exception
 END(page_fault)
 
diff --git a/arch/x86/entry/entry_64.S b/arch/x86/entry/entry_64.S
index db5009ce065a..e7b0e7ff4c58 100644
--- a/arch/x86/entry/entry_64.S
+++ b/arch/x86/entry/entry_64.S
@@ -36,8 +36,6 @@
 #include <asm/smap.h>
 #include <asm/pgtable_types.h>
 #include <asm/export.h>
-#include <asm/kaiser.h>
-#include <asm/nospec-branch.h>
 #include <linux/err.h>
 
 /* Avoid __ASSEMBLER__'ifying <linux/audit.h> just for this.  */
@@ -148,7 +146,6 @@ ENTRY(entry_SYSCALL_64)
 	 * it is too small to ever cause noticeable irq latency.
 	 */
 	SWAPGS_UNSAFE_STACK
-	SWITCH_KERNEL_CR3_NO_STACK
 	/*
 	 * A hypervisor implementation might want to use a label
 	 * after the swapgs, so that it can do the swapgs
@@ -177,17 +174,83 @@ GLOBAL(entry_SYSCALL_64_after_swapgs)
 	pushq	%r9				/* pt_regs->r9 */
 	pushq	%r10				/* pt_regs->r10 */
 	pushq	%r11				/* pt_regs->r11 */
-	pushq	%rbx				/* pt_regs->rbx */
-	pushq	%rbp				/* pt_regs->rbp */
-	pushq	%r12				/* pt_regs->r12 */
-	pushq	%r13				/* pt_regs->r13 */
-	pushq	%r14				/* pt_regs->r14 */
-	pushq	%r15				/* pt_regs->r15 */
+	sub	$(6*8), %rsp			/* pt_regs->bp, bx, r12-15 not saved */
 
+	/*
+	 * If we need to do entry work or if we guess we'll need to do
+	 * exit work, go straight to the slow path.
+	 */
+	movq	PER_CPU_VAR(current_task), %r11
+	testl	$_TIF_WORK_SYSCALL_ENTRY|_TIF_ALLWORK_MASK, TASK_TI_flags(%r11)
+	jnz	entry_SYSCALL64_slow_path
+
+entry_SYSCALL_64_fastpath:
+	/*
+	 * Easy case: enable interrupts and issue the syscall.  If the syscall
+	 * needs pt_regs, we'll call a stub that disables interrupts again
+	 * and jumps to the slow path.
+	 */
+	TRACE_IRQS_ON
+	ENABLE_INTERRUPTS(CLBR_NONE)
+#if __SYSCALL_MASK == ~0
+	cmpq	$__NR_syscall_max, %rax
+#else
+	andl	$__SYSCALL_MASK, %eax
+	cmpl	$__NR_syscall_max, %eax
+#endif
+	ja	1f				/* return -ENOSYS (already in pt_regs->ax) */
+	movq	%r10, %rcx
+
+	/*
+	 * This call instruction is handled specially in stub_ptregs_64.
+	 * It might end up jumping to the slow path.  If it jumps, RAX
+	 * and all argument registers are clobbered.
+	 */
+	call	*sys_call_table(, %rax, 8)
+.Lentry_SYSCALL_64_after_fastpath_call:
+
+	movq	%rax, RAX(%rsp)
+1:
+
+	/*
+	 * If we get here, then we know that pt_regs is clean for SYSRET64.
+	 * If we see that no exit work is required (which we are required
+	 * to check with IRQs off), then we can go straight to SYSRET64.
+	 */
+	DISABLE_INTERRUPTS(CLBR_NONE)
+	TRACE_IRQS_OFF
+	movq	PER_CPU_VAR(current_task), %r11
+	testl	$_TIF_ALLWORK_MASK, TASK_TI_flags(%r11)
+	jnz	1f
+
+	LOCKDEP_SYS_EXIT
+	TRACE_IRQS_ON		/* user mode is traced as IRQs on */
+	movq	RIP(%rsp), %rcx
+	movq	EFLAGS(%rsp), %r11
+	RESTORE_C_REGS_EXCEPT_RCX_R11
+	movq	RSP(%rsp), %rsp
+	USERGS_SYSRET64
+
+1:
+	/*
+	 * The fast path looked good when we started, but something changed
+	 * along the way and we need to switch to the slow path.  Calling
+	 * raise(3) will trigger this, for example.  IRQs are off.
+	 */
+	TRACE_IRQS_ON
+	ENABLE_INTERRUPTS(CLBR_NONE)
+	SAVE_EXTRA_REGS
+	movq	%rsp, %rdi
+	call	syscall_return_slowpath	/* returns with IRQs disabled */
+	jmp	return_from_SYSCALL_64
+
+entry_SYSCALL64_slow_path:
 	/* IRQs are off. */
+	SAVE_EXTRA_REGS
 	movq	%rsp, %rdi
 	call	do_syscall_64		/* returns with IRQs disabled */
 
+return_from_SYSCALL_64:
 	RESTORE_EXTRA_REGS
 	TRACE_IRQS_IRETQ		/* we're about to change IF */
 
@@ -260,31 +323,53 @@ GLOBAL(entry_SYSCALL_64_after_swapgs)
 syscall_return_via_sysret:
 	/* rcx and r11 are already restored (see code above) */
 	RESTORE_C_REGS_EXCEPT_RCX_R11
-
-	/*
-	 * This opens a window where we have a user CR3, but are
-	 * running in the kernel.  This makes using the CS
-	 * register useless for telling whether or not we need to
-	 * switch CR3 in NMIs.  Normal interrupts are OK because
-	 * they are off here.
-	 */
-	SWITCH_USER_CR3
 	movq	RSP(%rsp), %rsp
 	USERGS_SYSRET64
 
 opportunistic_sysret_failed:
-	/*
-	 * This opens a window where we have a user CR3, but are
-	 * running in the kernel.  This makes using the CS
-	 * register useless for telling whether or not we need to
-	 * switch CR3 in NMIs.  Normal interrupts are OK because
-	 * they are off here.
-	 */
-	SWITCH_USER_CR3
 	SWAPGS
 	jmp	restore_c_regs_and_iret
 END(entry_SYSCALL_64)
 
+ENTRY(stub_ptregs_64)
+	/*
+	 * Syscalls marked as needing ptregs land here.
+	 * If we are on the fast path, we need to save the extra regs,
+	 * which we achieve by trying again on the slow path.  If we are on
+	 * the slow path, the extra regs are already saved.
+	 *
+	 * RAX stores a pointer to the C function implementing the syscall.
+	 * IRQs are on.
+	 */
+	cmpq	$.Lentry_SYSCALL_64_after_fastpath_call, (%rsp)
+	jne	1f
+
+	/*
+	 * Called from fast path -- disable IRQs again, pop return address
+	 * and jump to slow path
+	 */
+	DISABLE_INTERRUPTS(CLBR_NONE)
+	TRACE_IRQS_OFF
+	popq	%rax
+	jmp	entry_SYSCALL64_slow_path
+
+1:
+	jmp	*%rax				/* Called from C */
+END(stub_ptregs_64)
+
+.macro ptregs_stub func
+ENTRY(ptregs_\func)
+	leaq	\func(%rip), %rax
+	jmp	stub_ptregs_64
+END(ptregs_\func)
+.endm
+
+/* Instantiate ptregs_stub for each ptregs-using syscall */
+#define __SYSCALL_64_QUAL_(sym)
+#define __SYSCALL_64_QUAL_ptregs(sym) ptregs_stub sym
+#define __SYSCALL_64(nr, sym, qual) __SYSCALL_64_QUAL_##qual(sym)
+#include <asm/syscalls_64.h>
+
 /*
  * %rdi: prev task
  * %rsi: next task
@@ -310,18 +395,6 @@ ENTRY(__switch_to_asm)
 	movq	%rbx, PER_CPU_VAR(irq_stack_union)+stack_canary_offset
 #endif
 
-#ifdef CONFIG_RETPOLINE
-	/*
-	 * When switching from a shallower to a deeper call stack
-	 * the RSB may either underflow or use entries populated
-	 * with userspace addresses. On CPUs where those concerns
-	 * exist, overwrite the RSB with entries which capture
-	 * speculative execution to prevent attack.
-	 */
-	/* Clobbers %rbx */
-	FILL_RETURN_BUFFER RSB_CLEAR_LOOPS, X86_FEATURE_RSB_CTXSW
-#endif
-
 	/* restore callee-saved registers */
 	popq	%r15
 	popq	%r14
@@ -351,14 +424,13 @@ ENTRY(ret_from_fork)
 	movq	%rsp, %rdi
 	call	syscall_return_slowpath	/* returns with IRQs disabled */
 	TRACE_IRQS_ON			/* user mode is traced as IRQS on */
-	SWITCH_USER_CR3
 	SWAPGS
 	jmp	restore_regs_and_iret
 
 1:
 	/* kernel thread */
 	movq	%r12, %rdi
-	CALL_NOSPEC %rbx
+	call	*%rbx
 	/*
 	 * A kernel thread is allowed to return here after successfully
 	 * calling do_execve().  Exit to userspace to complete the execve()
@@ -406,7 +478,6 @@ END(irq_entries_start)
 	 * tracking that we're in kernel mode.
 	 */
 	SWAPGS
-	SWITCH_KERNEL_CR3
 
 	/*
 	 * We need to tell lockdep that IRQs are off.  We can't do this until
@@ -464,7 +535,6 @@ GLOBAL(retint_user)
 	mov	%rsp,%rdi
 	call	prepare_exit_to_usermode
 	TRACE_IRQS_IRETQ
-	SWITCH_USER_CR3
 	SWAPGS
 	jmp	restore_regs_and_iret
 
@@ -542,7 +612,6 @@ native_irq_return_ldt:
 
 	pushq	%rdi				/* Stash user RDI */
 	SWAPGS
-	SWITCH_KERNEL_CR3
 	movq	PER_CPU_VAR(espfix_waddr), %rdi
 	movq	%rax, (0*8)(%rdi)		/* user RAX */
 	movq	(1*8)(%rsp), %rax		/* user RIP */
@@ -569,7 +638,6 @@ native_irq_return_ldt:
 	 * still points to an RO alias of the ESPFIX stack.
 	 */
 	orq	PER_CPU_VAR(espfix_stack), %rax
-	SWITCH_USER_CR3
 	SWAPGS
 	movq	%rax, %rsp
 
@@ -948,17 +1016,13 @@ idtentry async_page_fault	do_async_page_fault	has_error_code=1
 #endif
 
 #ifdef CONFIG_X86_MCE
-idtentry machine_check		do_mce			has_error_code=0	paranoid=1
+idtentry machine_check					has_error_code=0	paranoid=1 do_sym=*machine_check_vector(%rip)
 #endif
 
 /*
  * Save all registers in pt_regs, and switch gs if needed.
  * Use slow, but surefire "are we in kernel?" check.
- *
- * Return: ebx=0: needs swapgs but not SWITCH_USER_CR3 in paranoid_exit
- *         ebx=1: needs neither swapgs nor SWITCH_USER_CR3 in paranoid_exit
- *         ebx=2: needs both swapgs and SWITCH_USER_CR3 in paranoid_exit
- *         ebx=3: needs SWITCH_USER_CR3 but not swapgs in paranoid_exit
+ * Return: ebx=0: need swapgs on exit, ebx=1: otherwise
  */
 ENTRY(paranoid_entry)
 	cld
@@ -971,26 +1035,7 @@ ENTRY(paranoid_entry)
 	js	1f				/* negative -> in kernel */
 	SWAPGS
 	xorl	%ebx, %ebx
-1:
-#ifdef CONFIG_PAGE_TABLE_ISOLATION
-	/*
-	 * We might have come in between a swapgs and a SWITCH_KERNEL_CR3
-	 * on entry, or between a SWITCH_USER_CR3 and a swapgs on exit.
-	 * Do a conditional SWITCH_KERNEL_CR3: this could safely be done
-	 * unconditionally, but we need to find out whether the reverse
-	 * should be done on return (conveyed to paranoid_exit in %ebx).
-	 */
-	ALTERNATIVE "jmp 2f", "movq %cr3, %rax", X86_FEATURE_KAISER
-	testl	$KAISER_SHADOW_PGD_OFFSET, %eax
-	jz	2f
-	orl	$2, %ebx
-	andq	$(~(X86_CR3_PCID_ASID_MASK | KAISER_SHADOW_PGD_OFFSET)), %rax
-	/* If PCID enabled, set X86_CR3_PCID_NOFLUSH_BIT */
-	ALTERNATIVE "", "bts $63, %rax", X86_FEATURE_PCID
-	movq	%rax, %cr3
-2:
-#endif
-	ret
+1:	ret
 END(paranoid_entry)
 
 /*
@@ -1003,26 +1048,19 @@ END(paranoid_entry)
  * be complicated.  Fortunately, we there's no good reason
  * to try to handle preemption here.
  *
- * On entry: ebx=0: needs swapgs but not SWITCH_USER_CR3
- *           ebx=1: needs neither swapgs nor SWITCH_USER_CR3
- *           ebx=2: needs both swapgs and SWITCH_USER_CR3
- *           ebx=3: needs SWITCH_USER_CR3 but not swapgs
+ * On entry, ebx is "no swapgs" flag (1: don't need swapgs, 0: need it)
  */
 ENTRY(paranoid_exit)
 	DISABLE_INTERRUPTS(CLBR_NONE)
 	TRACE_IRQS_OFF_DEBUG
-	TRACE_IRQS_IRETQ_DEBUG
-#ifdef CONFIG_PAGE_TABLE_ISOLATION
-	/* No ALTERNATIVE for X86_FEATURE_KAISER: paranoid_entry sets %ebx */
-	testl	$2, %ebx			/* SWITCH_USER_CR3 needed? */
-	jz	paranoid_exit_no_switch
-	SWITCH_USER_CR3
-paranoid_exit_no_switch:
-#endif
-	testl	$1, %ebx			/* swapgs needed? */
+	testl	%ebx, %ebx			/* swapgs needed? */
 	jnz	paranoid_exit_no_swapgs
+	TRACE_IRQS_IRETQ
 	SWAPGS_UNSAFE_STACK
+	jmp	paranoid_exit_restore
 paranoid_exit_no_swapgs:
+	TRACE_IRQS_IRETQ_DEBUG
+paranoid_exit_restore:
 	RESTORE_EXTRA_REGS
 	RESTORE_C_REGS
 	REMOVE_PT_GPREGS_FROM_STACK 8
@@ -1037,13 +1075,6 @@ ENTRY(error_entry)
 	cld
 	SAVE_C_REGS 8
 	SAVE_EXTRA_REGS 8
-	/*
-	 * error_entry() always returns with a kernel gsbase and
-	 * CR3.  We must also have a kernel CR3/gsbase before
-	 * calling TRACE_IRQS_*.  Just unconditionally switch to
-	 * the kernel CR3 here.
-	 */
-	SWITCH_KERNEL_CR3
 	xorl	%ebx, %ebx
 	testb	$3, CS+8(%rsp)
 	jz	.Lerror_kernelspace
@@ -1204,10 +1235,6 @@ ENTRY(nmi)
 	 */
 
 	SWAPGS_UNSAFE_STACK
-	/*
-	 * percpu variables are mapped with user CR3, so no need
-	 * to switch CR3 here.
-	 */
 	cld
 	movq	%rsp, %rdx
 	movq	PER_CPU_VAR(cpu_current_top_of_stack), %rsp
@@ -1241,34 +1268,12 @@ ENTRY(nmi)
 
 	movq	%rsp, %rdi
 	movq	$-1, %rsi
-#ifdef CONFIG_PAGE_TABLE_ISOLATION
-	/* Unconditionally use kernel CR3 for do_nmi() */
-	/* %rax is saved above, so OK to clobber here */
-	ALTERNATIVE "jmp 2f", "movq %cr3, %rax", X86_FEATURE_KAISER
-	/* If PCID enabled, NOFLUSH now and NOFLUSH on return */
-	ALTERNATIVE "", "bts $63, %rax", X86_FEATURE_PCID
-	pushq	%rax
-	/* mask off "user" bit of pgd address and 12 PCID bits: */
-	andq	$(~(X86_CR3_PCID_ASID_MASK | KAISER_SHADOW_PGD_OFFSET)), %rax
-	movq	%rax, %cr3
-2:
-#endif
 	call	do_nmi
 
-#ifdef CONFIG_PAGE_TABLE_ISOLATION
-	/*
-	 * Unconditionally restore CR3.  I know we return to
-	 * kernel code that needs user CR3, but do we ever return
-	 * to "user mode" where we need the kernel CR3?
-	 */
-	ALTERNATIVE "", "popq %rax; movq %rax, %cr3", X86_FEATURE_KAISER
-#endif
-
 	/*
 	 * Return back to user mode.  We must *not* do the normal exit
-	 * work, because we don't want to enable interrupts.  Do not
-	 * switch to user CR3: we might be going back to kernel code
-	 * that had a user CR3 set.
+	 * work, because we don't want to enable interrupts.  Fortunately,
+	 * do_nmi doesn't modify pt_regs.
 	 */
 	SWAPGS
 	jmp	restore_c_regs_and_iret
@@ -1465,55 +1470,22 @@ end_repeat_nmi:
 	ALLOC_PT_GPREGS_ON_STACK
 
 	/*
-	 * Use the same approach as paranoid_entry to handle SWAPGS, but
-	 * without CR3 handling since we do that differently in NMIs.  No
-	 * need to use paranoid_exit as we should not be calling schedule
-	 * in NMI context.  Even with normal interrupts enabled. An NMI
-	 * should not be setting NEED_RESCHED or anything that normal
-	 * interrupts and exceptions might do.
+	 * Use paranoid_entry to handle SWAPGS, but no need to use paranoid_exit
+	 * as we should not be calling schedule in NMI context.
+	 * Even with normal interrupts enabled. An NMI should not be
+	 * setting NEED_RESCHED or anything that normal interrupts and
+	 * exceptions might do.
 	 */
-	cld
-	SAVE_C_REGS
-	SAVE_EXTRA_REGS
-	movl	$1, %ebx
-	movl	$MSR_GS_BASE, %ecx
-	rdmsr
-	testl	%edx, %edx
-	js	1f				/* negative -> in kernel */
-	SWAPGS
-	xorl	%ebx, %ebx
-1:
-	movq	%rsp, %rdi
-	movq	$-1, %rsi
-#ifdef CONFIG_PAGE_TABLE_ISOLATION
-	/* Unconditionally use kernel CR3 for do_nmi() */
-	/* %rax is saved above, so OK to clobber here */
-	ALTERNATIVE "jmp 2f", "movq %cr3, %rax", X86_FEATURE_KAISER
-	/* If PCID enabled, NOFLUSH now and NOFLUSH on return */
-	ALTERNATIVE "", "bts $63, %rax", X86_FEATURE_PCID
-	pushq	%rax
-	/* mask off "user" bit of pgd address and 12 PCID bits: */
-	andq	$(~(X86_CR3_PCID_ASID_MASK | KAISER_SHADOW_PGD_OFFSET)), %rax
-	movq	%rax, %cr3
-2:
-#endif
+	call	paranoid_entry
 
 	/* paranoidentry do_nmi, 0; without TRACE_IRQS_OFF */
+	movq	%rsp, %rdi
+	movq	$-1, %rsi
 	call	do_nmi
 
-#ifdef CONFIG_PAGE_TABLE_ISOLATION
-	/*
-	 * Unconditionally restore CR3.  We might be returning to
-	 * kernel code that needs user CR3, like just just before
-	 * a sysret.
-	 */
-	ALTERNATIVE "", "popq %rax; movq %rax, %cr3", X86_FEATURE_KAISER
-#endif
-
 	testl	%ebx, %ebx			/* swapgs needed? */
 	jnz	nmi_restore
 nmi_swapgs:
-	/* We fixed up CR3 above, so no need to switch it here */
 	SWAPGS_UNSAFE_STACK
 nmi_restore:
 	RESTORE_EXTRA_REGS
diff --git a/arch/x86/include/asm/thread_info.h b/arch/x86/include/asm/thread_info.h
index 89978b9c667a..ad6f5eb07a95 100644
--- a/arch/x86/include/asm/thread_info.h
+++ b/arch/x86/include/asm/thread_info.h
@@ -54,7 +54,6 @@ struct task_struct;
 
 struct thread_info {
 	unsigned long		flags;		/* low level flags */
-	u32			status;		/* thread synchronous flags */
 };
 
 #define INIT_THREAD_INFO(tsk)			\
@@ -153,6 +152,17 @@ struct thread_info {
  */
 #ifndef __ASSEMBLY__
 
+static inline unsigned long current_stack_pointer(void)
+{
+	unsigned long sp;
+#ifdef CONFIG_X86_64
+	asm("mov %%rsp,%0" : "=g" (sp));
+#else
+	asm("mov %%esp,%0" : "=g" (sp));
+#endif
+	return sp;
+}
+
 /*
  * Walks up the stack frames to make sure that the specified object is
  * entirely contained by a single stack frame.
@@ -214,7 +224,7 @@ static inline int arch_within_stack_frames(const void * const stack,
 #define in_ia32_syscall() true
 #else
 #define in_ia32_syscall() (IS_ENABLED(CONFIG_IA32_EMULATION) && \
-			   current_thread_info()->status & TS_COMPAT)
+			   current->thread.status & TS_COMPAT)
 #endif
 
 /*
diff --git a/drivers/gpio/gpio-davinci.c b/drivers/gpio/gpio-davinci.c
index e9440de6e7a9..dd262f00295d 100644
--- a/drivers/gpio/gpio-davinci.c
+++ b/drivers/gpio/gpio-davinci.c
@@ -42,7 +42,25 @@ typedef struct irq_chip *(*gpio_get_irq_chip_cb_t)(unsigned int irq);
 #define BINTEN	0x8 /* GPIO Interrupt Per-Bank Enable Register */
 
 static void __iomem *gpio_base;
-static unsigned int offset_array[5] = {0x10, 0x38, 0x60, 0x88, 0xb0};
+
+static struct davinci_gpio_regs __iomem *gpio2regs(unsigned gpio)
+{
+	void __iomem *ptr;
+
+	if (gpio < 32 * 1)
+		ptr = gpio_base + 0x10;
+	else if (gpio < 32 * 2)
+		ptr = gpio_base + 0x38;
+	else if (gpio < 32 * 3)
+		ptr = gpio_base + 0x60;
+	else if (gpio < 32 * 4)
+		ptr = gpio_base + 0x88;
+	else if (gpio < 32 * 5)
+		ptr = gpio_base + 0xb0;
+	else
+		ptr = NULL;
+	return ptr;
+}
 
 static inline struct davinci_gpio_regs __iomem *irq2regs(struct irq_data *d)
 {
@@ -62,13 +80,10 @@ static inline int __davinci_direction(struct gpio_chip *chip,
 			unsigned offset, bool out, int value)
 {
 	struct davinci_gpio_controller *d = gpiochip_get_data(chip);
-	struct davinci_gpio_regs __iomem *g;
+	struct davinci_gpio_regs __iomem *g = d->regs;
 	unsigned long flags;
 	u32 temp;
-	int bank = offset / 32;
-	u32 mask = __gpio_mask(offset);
-
-	g = d->regs[bank];
+	u32 mask = 1 << offset;
 
 	spin_lock_irqsave(&d->lock, flags);
 	temp = readl_relaxed(&g->dir);
@@ -105,12 +120,9 @@ davinci_direction_out(struct gpio_chip *chip, unsigned offset, int value)
 static int davinci_gpio_get(struct gpio_chip *chip, unsigned offset)
 {
 	struct davinci_gpio_controller *d = gpiochip_get_data(chip);
-	struct davinci_gpio_regs __iomem *g;
-	int bank = offset / 32;
-
-	g = d->regs[bank];
+	struct davinci_gpio_regs __iomem *g = d->regs;
 
-	return !!(__gpio_mask(offset) & readl_relaxed(&g->in_data));
+	return !!((1 << offset) & readl_relaxed(&g->in_data));
 }
 
 /*
@@ -120,13 +132,9 @@ static void
 davinci_gpio_set(struct gpio_chip *chip, unsigned offset, int value)
 {
 	struct davinci_gpio_controller *d = gpiochip_get_data(chip);
-	struct davinci_gpio_regs __iomem *g;
-	int bank = offset / 32;
+	struct davinci_gpio_regs __iomem *g = d->regs;
 
-	g = d->regs[bank];
-
-	writel_relaxed(__gpio_mask(offset),
-		       value ? &g->set_data : &g->clr_data);
+	writel_relaxed((1 << offset), value ? &g->set_data : &g->clr_data);
 }
 
 static struct davinci_gpio_platform_data *
@@ -163,15 +171,36 @@ davinci_gpio_get_pdata(struct platform_device *pdev)
 	return NULL;
 }
 
+#ifdef CONFIG_OF_GPIO
+static int davinci_gpio_of_xlate(struct gpio_chip *gc,
+			     const struct of_phandle_args *gpiospec,
+			     u32 *flags)
+{
+	struct davinci_gpio_controller *chips = dev_get_drvdata(gc->parent);
+	struct davinci_gpio_platform_data *pdata = dev_get_platdata(gc->parent);
+
+	if (gpiospec->args[0] > pdata->ngpio)
+		return -EINVAL;
+
+	if (gc != &chips[gpiospec->args[0] / 32].chip)
+		return -EINVAL;
+
+	if (flags)
+		*flags = gpiospec->args[1];
+
+	return gpiospec->args[0] % 32;
+}
+#endif
+
 static int davinci_gpio_probe(struct platform_device *pdev)
 {
-	int gpio, bank;
+	int i, base;
 	unsigned ngpio, nbank;
 	struct davinci_gpio_controller *chips;
 	struct davinci_gpio_platform_data *pdata;
+	struct davinci_gpio_regs __iomem *regs;
 	struct device *dev = &pdev->dev;
 	struct resource *res;
-	static int bank_base;
 
 	pdata = davinci_gpio_get_pdata(pdev);
 	if (!pdata) {
@@ -207,26 +236,38 @@ static int davinci_gpio_probe(struct platform_device *pdev)
 	if (IS_ERR(gpio_base))
 		return PTR_ERR(gpio_base);
 
-	chips->chip.label = "Davinci";
-	chips->chip.direction_input = davinci_direction_in;
-	chips->chip.get = davinci_gpio_get;
-	chips->chip.direction_output = davinci_direction_out;
-	chips->chip.set = davinci_gpio_set;
-	chips->chip.ngpio = ngpio;
-	chips->chip.base = bank_base;
+	for (i = 0, base = 0; base < ngpio; i++, base += 32) {
+		chips[i].chip.label = "DaVinci";
+
+		chips[i].chip.direction_input = davinci_direction_in;
+		chips[i].chip.get = davinci_gpio_get;
+		chips[i].chip.direction_output = davinci_direction_out;
+		chips[i].chip.set = davinci_gpio_set;
+
+		chips[i].chip.base = base;
+		chips[i].chip.ngpio = ngpio - base;
+		if (chips[i].chip.ngpio > 32)
+			chips[i].chip.ngpio = 32;
 
 #ifdef CONFIG_OF_GPIO
-	chips->chip.of_gpio_n_cells = 2;
-	chips->chip.parent = dev;
-	chips->chip.of_node = dev->of_node;
+		chips[i].chip.of_gpio_n_cells = 2;
+		chips[i].chip.of_xlate = davinci_gpio_of_xlate;
+		chips[i].chip.parent = dev;
+		chips[i].chip.of_node = dev->of_node;
 #endif
-	spin_lock_init(&chips->lock);
-	bank_base += ngpio;
+		spin_lock_init(&chips[i].lock);
+
+		regs = gpio2regs(base);
+		if (!regs)
+			return -ENXIO;
+		chips[i].regs = regs;
+		chips[i].set_data = &regs->set_data;
+		chips[i].clr_data = &regs->clr_data;
+		chips[i].in_data = &regs->in_data;
 
-	for (gpio = 0, bank = 0; gpio < ngpio; gpio += 32, bank++)
-		chips->regs[bank] = gpio_base + offset_array[bank];
+		gpiochip_add_data(&chips[i].chip, &chips[i]);
+	}
 
-	gpiochip_add_data(&chips->chip, chips);
 	platform_set_drvdata(pdev, chips);
 	davinci_gpio_irq_setup(pdev);
 	return 0;
@@ -287,19 +328,16 @@ static struct irq_chip gpio_irqchip = {
 
 static void gpio_irq_handler(struct irq_desc *desc)
 {
+	unsigned int irq = irq_desc_get_irq(desc);
 	struct davinci_gpio_regs __iomem *g;
 	u32 mask = 0xffff;
-	int bank_num;
 	struct davinci_gpio_controller *d;
-	struct davinci_gpio_irq_data *irqdata;
 
-	irqdata = (struct davinci_gpio_irq_data *)irq_desc_get_handler_data(desc);
-	bank_num = irqdata->bank_num;
-	g = irqdata->regs;
-	d = irqdata->chip;
+	d = (struct davinci_gpio_controller *)irq_desc_get_handler_data(desc);
+	g = (struct davinci_gpio_regs __iomem *)d->regs;
 
 	/* we only care about one bank */
-	if ((bank_num % 2) == 1)
+	if (irq & 1)
 		mask <<= 16;
 
 	/* temporarily mask (level sensitive) parent IRQ */
@@ -307,7 +345,6 @@ static void gpio_irq_handler(struct irq_desc *desc)
 	while (1) {
 		u32		status;
 		int		bit;
-		irq_hw_number_t hw_irq;
 
 		/* ack any irqs */
 		status = readl_relaxed(&g->intstat) & mask;
@@ -320,13 +357,9 @@ static void gpio_irq_handler(struct irq_desc *desc)
 		while (status) {
 			bit = __ffs(status);
 			status &= ~BIT(bit);
-			/* Max number of gpios per controller is 144 so
-			 * hw_irq will be in [0..143]
-			 */
-			hw_irq = (bank_num / 2) * 32 + bit;
-
 			generic_handle_irq(
-				irq_find_mapping(d->irq_domain, hw_irq));
+				irq_find_mapping(d->irq_domain,
+						 d->chip.base + bit));
 		}
 	}
 	chained_irq_exit(irq_desc_get_chip(desc), desc);
@@ -338,7 +371,7 @@ static int gpio_to_irq_banked(struct gpio_chip *chip, unsigned offset)
 	struct davinci_gpio_controller *d = gpiochip_get_data(chip);
 
 	if (d->irq_domain)
-		return irq_create_mapping(d->irq_domain, offset);
+		return irq_create_mapping(d->irq_domain, d->chip.base + offset);
 	else
 		return -ENXIO;
 }
@@ -352,7 +385,7 @@ static int gpio_to_irq_unbanked(struct gpio_chip *chip, unsigned offset)
 	 * can provide direct-mapped IRQs to AINTC (up to 32 GPIOs).
 	 */
 	if (offset < d->gpio_unbanked)
-		return d->base_irq + offset;
+		return d->gpio_irq + offset;
 	else
 		return -ENODEV;
 }
@@ -364,8 +397,8 @@ static int gpio_irq_type_unbanked(struct irq_data *data, unsigned trigger)
 	u32 mask;
 
 	d = (struct davinci_gpio_controller *)irq_data_get_irq_handler_data(data);
-	g = (struct davinci_gpio_regs __iomem *)d->regs[0];
-	mask = __gpio_mask(data->irq - d->base_irq);
+	g = (struct davinci_gpio_regs __iomem *)d->regs;
+	mask = __gpio_mask(data->irq - d->gpio_irq);
 
 	if (trigger & ~(IRQ_TYPE_EDGE_FALLING | IRQ_TYPE_EDGE_RISING))
 		return -EINVAL;
@@ -382,9 +415,7 @@ static int
 davinci_gpio_irq_map(struct irq_domain *d, unsigned int irq,
 		     irq_hw_number_t hw)
 {
-	struct davinci_gpio_controller *chips =
-				(struct davinci_gpio_controller *)d->host_data;
-	struct davinci_gpio_regs __iomem *g = chips->regs[hw / 32];
+	struct davinci_gpio_regs __iomem *g = gpio2regs(hw);
 
 	irq_set_chip_and_handler_name(irq, &gpio_irqchip, handle_simple_irq,
 				"davinci_gpio");
@@ -419,26 +450,6 @@ static struct irq_chip *keystone_gpio_get_irq_chip(unsigned int irq)
 
 static const struct of_device_id davinci_gpio_ids[];
 
-struct gpio_driver_data {
-	gpio_get_irq_chip_cb_t gpio_get_irq_chip;
-	bool clk_optional;
-};
-
-static struct gpio_driver_data davinci_data = {
-	.gpio_get_irq_chip = davinci_gpio_get_irq_chip,
-	.clk_optional = false,
-};
-
-static struct gpio_driver_data keystone_data = {
-	.gpio_get_irq_chip = keystone_gpio_get_irq_chip,
-	.clk_optional = false,
-};
-
-static struct gpio_driver_data k2g_data = {
-	.gpio_get_irq_chip = keystone_gpio_get_irq_chip,
-	.clk_optional = true,
-};
-
 /*
  * NOTE:  for suspend/resume, probably best to make a platform_device with
  * suspend_late/resume_resume calls hooking into results of the set_wake()
@@ -462,8 +473,6 @@ static int davinci_gpio_irq_setup(struct platform_device *pdev)
 	struct irq_domain	*irq_domain = NULL;
 	const struct of_device_id *match;
 	struct irq_chip *irq_chip;
-	struct davinci_gpio_irq_data *irqdata;
-	struct gpio_driver_data *driver_data = NULL;
 	gpio_get_irq_chip_cb_t gpio_get_irq_chip;
 
 	/*
@@ -472,10 +481,8 @@ static int davinci_gpio_irq_setup(struct platform_device *pdev)
 	gpio_get_irq_chip = davinci_gpio_get_irq_chip;
 	match = of_match_device(of_match_ptr(davinci_gpio_ids),
 				dev);
-	if (match) {
-		driver_data = (struct gpio_driver_data *)match->data;
-		gpio_get_irq_chip = driver_data->gpio_get_irq_chip;
-	}
+	if (match)
+		gpio_get_irq_chip = (gpio_get_irq_chip_cb_t)match->data;
 
 	ngpio = pdata->ngpio;
 	res = platform_get_resource(pdev, IORESOURCE_IRQ, 0);
@@ -491,9 +498,6 @@ static int davinci_gpio_irq_setup(struct platform_device *pdev)
 		return -ENODEV;
 	}
 
-	if (driver_data && driver_data->clk_optional)
-		goto skip_clk_handling;
-
 	clk = devm_clk_get(dev, "gpio");
 	if (IS_ERR(clk)) {
 		printk(KERN_ERR "Error %ld getting gpio clock?\n",
@@ -502,7 +506,6 @@ static int davinci_gpio_irq_setup(struct platform_device *pdev)
 	}
 	clk_prepare_enable(clk);
 
-skip_clk_handling:
 	if (!pdata->gpio_unbanked) {
 		irq = irq_alloc_descs(-1, 0, ngpio, 0);
 		if (irq < 0) {
@@ -525,8 +528,10 @@ static int davinci_gpio_irq_setup(struct platform_device *pdev)
 	 * IRQs, while the others use banked IRQs, would need some setup
 	 * tweaks to recognize hardware which can do that.
 	 */
-	chips->chip.to_irq = gpio_to_irq_banked;
-	chips->irq_domain = irq_domain;
+	for (gpio = 0, bank = 0; gpio < ngpio; bank++, gpio += 32) {
+		chips[bank].chip.to_irq = gpio_to_irq_banked;
+		chips[bank].irq_domain = irq_domain;
+	}
 
 	/*
 	 * AINTC can handle direct/unbanked IRQs for GPIOs, with the GPIO
@@ -535,9 +540,9 @@ static int davinci_gpio_irq_setup(struct platform_device *pdev)
 	 */
 	if (pdata->gpio_unbanked) {
 		/* pass "bank 0" GPIO IRQs to AINTC */
-		chips->chip.to_irq = gpio_to_irq_unbanked;
-		chips->base_irq = bank_irq;
-		chips->gpio_unbanked = pdata->gpio_unbanked;
+		chips[0].chip.to_irq = gpio_to_irq_unbanked;
+		chips[0].gpio_irq = bank_irq;
+		chips[0].gpio_unbanked = pdata->gpio_unbanked;
 		binten = GENMASK(pdata->gpio_unbanked / 16, 0);
 
 		/* AINTC handles mask/unmask; GPIO handles triggering */
@@ -547,14 +552,14 @@ static int davinci_gpio_irq_setup(struct platform_device *pdev)
 		irq_chip->irq_set_type = gpio_irq_type_unbanked;
 
 		/* default trigger: both edges */
-		g = chips->regs[0];
+		g = gpio2regs(0);
 		writel_relaxed(~0, &g->set_falling);
 		writel_relaxed(~0, &g->set_rising);
 
 		/* set the direct IRQs up to use that irqchip */
 		for (gpio = 0; gpio < pdata->gpio_unbanked; gpio++, irq++) {
 			irq_set_chip(irq, irq_chip);
-			irq_set_handler_data(irq, chips);
+			irq_set_handler_data(irq, &chips[gpio / 32]);
 			irq_set_status_flags(irq, IRQ_TYPE_EDGE_BOTH);
 		}
 
@@ -567,7 +572,7 @@ static int davinci_gpio_irq_setup(struct platform_device *pdev)
 	 */
 	for (gpio = 0, bank = 0; gpio < ngpio; bank++, bank_irq++, gpio += 16) {
 		/* disabled by default, enabled only as needed */
-		g = chips->regs[bank / 2];
+		g = gpio2regs(gpio);
 		writel_relaxed(~0, &g->clr_falling);
 		writel_relaxed(~0, &g->clr_rising);
 
@@ -576,19 +581,8 @@ static int davinci_gpio_irq_setup(struct platform_device *pdev)
 		 * gpio irqs. Pass the irq bank's corresponding controller to
 		 * the chained irq handler.
 		 */
-		irqdata = devm_kzalloc(&pdev->dev,
-				       sizeof(struct
-					      davinci_gpio_irq_data),
-					      GFP_KERNEL);
-		if (!irqdata)
-			return -ENOMEM;
-
-		irqdata->regs = g;
-		irqdata->bank_num = bank;
-		irqdata->chip = chips;
-
 		irq_set_chained_handler_and_data(bank_irq, gpio_irq_handler,
-						 irqdata);
+						 &chips[gpio / 32]);
 
 		binten |= BIT(bank);
 	}
@@ -605,18 +599,8 @@ static int davinci_gpio_irq_setup(struct platform_device *pdev)
 
 #if IS_ENABLED(CONFIG_OF)
 static const struct of_device_id davinci_gpio_ids[] = {
-	{
-		.compatible = "ti,keystone-gpio",
-		.data = &keystone_data,
-	},
-	{
-		.compatible = "ti,dm6441-gpio",
-		.data = &davinci_data,
-	},
-	{
-		.compatible = "ti,k2g-gpio",
-		.data = &k2g_data,
-	},
+	{ .compatible = "ti,keystone-gpio", keystone_gpio_get_irq_chip},
+	{ .compatible = "ti,dm6441-gpio", davinci_gpio_get_irq_chip},
 	{ /* sentinel */ },
 };
 MODULE_DEVICE_TABLE(of, davinci_gpio_ids);
-- 
2.16.1

